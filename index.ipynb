{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Normalization - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this lab, you'll practice your feature scaling and normalization skills!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "You will be able to:\n",
    "* Implement min-max scaling, mean-normalization, log normalization and unit vector normalization in python\n",
    "* Identify appropriate normalization and scaling techniques for given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our Boston Housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our Boston Housing data. Remember we categorized two variables and deleted the \"NOX\" (nitride oxide concentration) variable because it was highly correlated with two other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "boston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "\n",
    "# first, create bins for based on the values observed. 5 values will result in 4 bins\n",
    "bins = [0, 3, 4 , 5, 24]\n",
    "bins_rad = pd.cut(boston_features['RAD'], bins)\n",
    "bins_rad = bins_rad.cat.as_unordered()\n",
    "\n",
    "# first, create bins for based on the values observed. 5 values will result in 4 bins\n",
    "bins = [0, 250, 300, 360, 460, 712]\n",
    "bins_tax = pd.cut(boston_features['TAX'], bins)\n",
    "bins_tax = bins_tax.cat.as_unordered()\n",
    "\n",
    "tax_dummy = pd.get_dummies(bins_tax, prefix=\"TAX\")\n",
    "rad_dummy = pd.get_dummies(bins_rad, prefix=\"RAD\")\n",
    "boston_features = boston_features.drop([\"RAD\",\"TAX\"], axis=1)\n",
    "boston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\n",
    "boston_features = boston_features.drop(\"NOX\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the histograms for the continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= boston_features\n",
    "boston_cont = df[df.columns.drop(list(df.filter(regex='TAX')))]\n",
    "boston_cont = boston_cont[boston_cont.columns.drop(list(boston_cont.filter(regex='RAD')))]\n",
    "boston_cont= boston_cont.drop(['CHAS'], axis=1)\n",
    "\n",
    "boston_cont.hist(figsize  = [8, 8]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform log transformations for the variables where it makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the results in terms of how they improved the normality performance. What is the problem with the \"ZN\" variable?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "boston_log = pd.DataFrame([])\n",
    "boston_log['AGE'] = np.log(boston_cont['AGE'])\n",
    "boston_log['B'] = np.log(boston_cont['B'])\n",
    "boston_log['CRIM'] = np.log(boston_cont['CRIM'])\n",
    "boston_log['DIS'] = np.log(boston_cont['DIS'])\n",
    "boston_log['INDUS'] = np.log(boston_cont['INDUS'])\n",
    "boston_log['LSTAT'] = np.log(boston_cont['LSTAT'])\n",
    "boston_log['PTRATIO'] = np.log(boston_cont['PTRATIO'])\n",
    "boston_log['ZN'] = np.log(boston_cont['ZN'])\n",
    "boston_log.hist(figsize  = [6, 6]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"ZN\" has a lot of zeros (more than 50%!). Remember that this variable denoted: \"proportion of residential land zoned for lots over 25,000 sq.ft.\". It might have made sense to categorize this variable to \"over 25,000 feet or not (binary variable 1/0). Now you have a zero-inflated variable which is cumbersome to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different types of transformations on the continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store your final features in a dataframe `features_final`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped ZN - cumbersome to work with and don't know enough transformations yet\n",
    "\n",
    "rm = boston_cont['RM'] # this was already well distributed \n",
    "age = boston_cont['AGE']\n",
    "b = boston_cont['B']\n",
    "# kept age and b non-log because the log transformation did not improve skewness\n",
    "logcrim = boston_log[\"CRIM\"] \n",
    "logdis = boston_log[\"DIS\"]  \n",
    "logindus = boston_log[\"INDUS\"] \n",
    "loglstat = boston_log[\"LSTAT\"]\n",
    "logptratio = boston_log[\"PTRATIO\"]\n",
    "\n",
    "\n",
    "scaled_rm = (rm-min(rm))/(max(rm)-min(rm)) # min-max\n",
    "scaled_age = (age-np.mean(age))/(max(age)-min(age)) # mean norm\n",
    "scaled_b = (b-min(b))/(max(b)-min(b)) # min-max\n",
    "scaled_crim = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim)) # min-max\n",
    "scaled_dis = (logdis-np.mean(logdis))/np.sqrt(np.var(logdis)) # standardization\n",
    "scaled_indus = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus)) # standardization\n",
    "scaled_lstat = (loglstat-np.mean(loglstat))/(max(loglstat)-min(loglstat)) # mean norm\n",
    "scaled_ptratio = (logptratio-min(logptratio))/(max(logptratio)-min(logptratio)) # min-max\n",
    "\n",
    "features_final = pd.DataFrame([])\n",
    "features_final['RM'] = scaled_rm\n",
    "features_final['AGE'] = scaled_age\n",
    "features_final['B'] = scaled_b\n",
    "features_final['CRIM'] = scaled_crim\n",
    "features_final['DIS'] = scaled_dis\n",
    "features_final['INDUS'] = scaled_indus\n",
    "features_final['LSTAT'] = scaled_lstat\n",
    "features_final['PTRATIO'] = scaled_ptratio\n",
    "features_final.hist(figsize  = [8, 8]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Great! You've now transformed your final data using feature scaling and normalization, and stored them in the `features_final` dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
